{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Iris Classification Problem",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgcXbUwle_nK"
      },
      "source": [
        "> ### RIIAA 2.0 – Workshop \n",
        "> **Deep Learning as a Service** <br>\n",
        "> **Instructor:** [Rodolfo Ferro](https://rodolfoferro.xyz) <br>\n",
        "> **Email:** <ferro@cimat.mx> <br>\n",
        "> **Twitter:** <https://twitter.com/FerroRodolfo/> <br>\n",
        "> **GitHub:** <https://github.com/RodolfoFerro/> <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqZboFApBN18"
      },
      "source": [
        "# Iris Classification Problem\n",
        "\n",
        "Along this notebook we'll explain how to use the power of cloud computing with Google Colab for a classical example –*The Iris Classification Problem*– using the popular [Iris flower dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set).\n",
        "\n",
        "For this classification problem we will build a simple feed-forward full-connected artificial neural network.\n",
        "\n",
        "The Python framework that we will be using is [Tensorflow 2.0](https://www.tensorflow.org) with the [Keras](https://keras.io/) module.\n",
        "\n",
        "\n",
        "### Problem statement\n",
        "\n",
        "Before we tackle the problem an ANN, let's understand what we'll be doing: \n",
        "\n",
        "* If we feed our neural network with Iris data, the model should be able to determine what species it is.\n",
        "\n",
        "> #### What do we need to do?\n",
        "> Train a _Deep Learning_ model (in this case) using a known dataset: [Iris flower dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set).\n",
        ">\n",
        "> Specifically, we are going to do the following:\n",
        "> - Load the dataset\n",
        "> - Preprocess the data\n",
        "> - Build the model\n",
        "> - Set hyperparameters \n",
        "> - Train the model\n",
        "> - Save and download the trained model\n",
        "> - Predict data\n",
        "\n",
        "## Installing dependencies\n",
        "\n",
        "For our training we will be using Tensorflow 2.0, so we want to be sure it is installed on its latest version:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7D74gMYrD0Fo"
      },
      "source": [
        "# Let's install Tensorflow 2.0:\n",
        "!pip install -q tensorflow==2.0.0-rc0\n",
        "\n",
        "# And verify that it is now in its latest version:\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ctjdb9BD0dP"
      },
      "source": [
        "## The Iris dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uN3Ro-5BKUq"
      },
      "source": [
        "from IPython.display import HTML\n",
        "url = 'https://en.wikipedia.org/wiki/Iris_flower_data_set'\n",
        "iframe = '<iframe src=' + url + ' width=\"100%\" height=400></iframe>'\n",
        "HTML(iframe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuiBrwaOCBbt"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDoI3H0_B7oF"
      },
      "source": [
        "# Importing dataset from scikit-learn and other useful packages:\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# We will fix a random seed for reproducibility:\n",
        "seed = 11\n",
        "np.random.seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtH71K6vDRff"
      },
      "source": [
        "# We now import the Iris dataset:\n",
        "iris = ???\n",
        "\n",
        "# And set the features and labels vectors from it:\n",
        "# Set x as 'data' from iris\n",
        "# Set y as 'target' from iris\n",
        "# Set names as 'target_names' from iris\n",
        "# Set feature_names as 'feature_names' from iris\n",
        "\n",
        "# We can load some elements to verify the contents in the dataset:\n",
        "elements_to_display = [???]\n",
        "for element in elements_to_display:\n",
        "    print(f\"Element {element}th:\")\n",
        "    print(f\"  - Features: {x[element]}\")\n",
        "    print(f\"  - Target: {y[element]}\")\n",
        "    print(f\"  - Species: {names[element % 3]}\")\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMRI1V43HZg9"
      },
      "source": [
        "## Preprocess dataset\n",
        "\n",
        "The preprocess step results very important in many cases. For this case, we will just need to do a very simple transformation: a one hot encode process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2_d1fQ-HKfI"
      },
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "# One hot encode outputs: \n",
        "y = keras.utils.to_categorical(y)\n",
        "\n",
        "# Set global variables:\n",
        "n_features = len(feature_names)\n",
        "n_classes = names.shape[0]\n",
        "\n",
        "# Let's checkout changes:\n",
        "for element in elements_to_display:\n",
        "    print(f\"Element {element}th:\")\n",
        "    print(f\"  - Features: {x[element]}\")\n",
        "    print(f\"  - Target: {y[element]}\")\n",
        "    print(f\"  - Species: {names[element % 3]}\")\n",
        "    print()\n",
        "\n",
        "    \n",
        "# Split the data set into training and testing sets:\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, \n",
        "                                                    test_size=0.3, \n",
        "                                                    random_state=seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipLTk1GEIWq_"
      },
      "source": [
        "## Let's talk about the model...\n",
        "\n",
        "We will be using a very simple model, a feed-forward multi-layer perceptron.\n",
        "\n",
        "### Let's create the model with Keras!\n",
        "\n",
        "First of all, let's import what we'll use:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6n42gKjsIBj8"
      },
      "source": [
        "# Let's import our Keras stuff:\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "def iris_model(input_dim, output_dim, init_nodes=4, name='model'):\n",
        "    \"\"\"FF-MLP model for Iris classification problem.\"\"\"\n",
        "    \n",
        "    # Create model:\n",
        "    model = Sequential(name=name)\n",
        "    # Add Dense -> init_nodes, input_dim=input_dim, activation='relu'\n",
        "    # Add Dense -> 2*init_nodes, activation='relu'\n",
        "    # Add Dense -> 3*init_nodes, activation='relu'\n",
        "    # Add Dense -> output_dim, activation='softmax'\n",
        "    \n",
        "    # Compile model:\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cgyhm_0dMh0R"
      },
      "source": [
        "### Useful resources\n",
        "\n",
        "- Sequential model: <https://keras.io/getting-started/sequential-model-guide/>\n",
        "- Classifying the Iris Data Set with Keras: <https://janakiev.com/notebooks/keras-iris/>\n",
        "\n",
        "### Building the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4ysBBcKMYhZ"
      },
      "source": [
        "# Let's build our model:\n",
        "model = ???\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HUigT2jMyQy"
      },
      "source": [
        "### Training the model\n",
        "\n",
        "In order to train the model, we first need to set its training hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdENafOPMsbA"
      },
      "source": [
        "# Set hyperparameters\n",
        "epochs = ???\n",
        "batch = ???\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(x_train, y_train, \n",
        "                    validation_data=(x_test, y_test),\n",
        "                    verbose=True,\n",
        "                    epochs=epochs, batch_size=batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMxNyoxvOV4D"
      },
      "source": [
        "### Evaluating the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLcDoeWxOU60"
      },
      "source": [
        "# Final evaluation of the model:\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print(f'Test accuracy: {scores[1]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qByC6HBbQjiZ"
      },
      "source": [
        "### Plot the training along the time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYoJzOAnQi97"
      },
      "source": [
        "def plot_loss(history):\n",
        "    plt.style.use(\"ggplot\")\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title(\"Model's training loss\")\n",
        "    plt.xlabel(\"Epoch #\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(['Train', 'Test'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_accuracy(history):\n",
        "    plt.style.use(\"ggplot\")\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title(\"Model's training accuracy\")\n",
        "    plt.xlabel(\"Epoch #\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend(['Train', 'Test'], loc='upper left')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kdabtBjNKwK"
      },
      "source": [
        "plot_loss(history)\n",
        "plot_accuracy(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHYbaMS2UhVZ"
      },
      "source": [
        "_How can we save these plots?_\n",
        "\n",
        "## Saving a model\n",
        "\n",
        "To save the trained model we will basically do two things:\n",
        "\n",
        "1. Serialize the model into a JSON file, which will save the architecture of our model.\n",
        "2. Serialize the weights into a HDF5 file, which will save all parameters of our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kq7y3CtkQnTT"
      },
      "source": [
        "# Serialize model to JSON:\n",
        "model_json = model.to_json()\n",
        "with open(\"iris_model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# Serialize weights to HDF5 (h5py needed):\n",
        "model.save_weights(\"iris_model.h5\")\n",
        "print(\"Model saved to disk.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJ1OGbhTVo82"
      },
      "source": [
        "## Downloading a model\n",
        "\n",
        "We just need to import the Google Colab module and download the specified files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZ9Z34ndVwvv"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "model_files = ['iris_model.json', 'iris_model.h5']\n",
        "for file in model_files:\n",
        "    files.download(file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzHznMlxXgry"
      },
      "source": [
        "## Loading a trained model\n",
        "We will basically do three things:\n",
        "\n",
        "1. Load the model from a JSON file.\n",
        "2. Load the weights from a HDF5 file.\n",
        "3. (Re)Compile the trained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZtlTbNpWdAM"
      },
      "source": [
        "# Load json and create model:\n",
        "from tensorflow.keras.models import model_from_json\n",
        "\n",
        "json_file = open('iris_model.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "\n",
        "# Load weights into loaded model:\n",
        "loaded_model.load_weights(\"iris_model.h5\")\n",
        "print(\"Model loaded from disk.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qnt7JaIGXxk8"
      },
      "source": [
        "# Evaluate loaded model on test data:\n",
        "loaded_model.compile(loss='categorical_crossentropy',\n",
        "                     optimizer='adam',\n",
        "                     metrics=['accuracy'])\n",
        "\n",
        "score = loaded_model.evaluate(x_test, y_test, verbose=1)\n",
        "print(f'Test accuracy: {score[1]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdg_v2slX9Ou"
      },
      "source": [
        "## Predicting from new data\n",
        "\n",
        "Now that we have a trained model, how do we use it?\n",
        "\n",
        "It is as simple as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7fUV0QyX5sB"
      },
      "source": [
        "# Remembering some elements:\n",
        "for element in elements_to_display:\n",
        "    prediction_vector = model.predict(np.array([x[element]]))\n",
        "    print(f\"Element {element}th:\")\n",
        "    print(f\"  - Features: {x[element]}\")\n",
        "    print(f\"  - Target: {y[element]}\")\n",
        "    print(f\"  - Scpecies: {names[np.argmax(y[element])]}\")\n",
        "    print(f\"  - Predicted species: {names[np.argmax(prediction_vector)]}\")\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVA5PIfXYWc2"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}